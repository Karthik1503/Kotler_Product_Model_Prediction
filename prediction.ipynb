import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from nltk.util import ngrams
import nltk
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from transformers import pipeline
from sklearn.cluster import KMeans


# Download necessary NLTK resources
nltk.download('stopwords')
stop_words = set(nltk.corpus.stopwords.words('english'))

def fetch_reviews(product_id):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    all_reviews = []
    
    for page in range(1, 10):  # Scraping first 4 pages of reviews
        url = f"https://www.amazon.in/product-reviews/{product_id}/ref=cm_cr_getr_d_paging_btm_next_{page}?pageNumber={page}"
        print(url)
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        reviews = soup.find_all('span', {'data-hook': 'review-body'})
        
        for review in reviews:
            all_reviews.append(review.text.strip())
    
    return pd.DataFrame(all_reviews, columns=['review'])

def clean_text(text):
    text = BeautifulSoup(text, 'html.parser').get_text()  # removing HTML tags
    text = ''.join([char for char in text if char.isalnum() or char.isspace()])
    # text = ' '.join([word for word in text.lower().split() if word not in stop_words])  # lowercasing and removing stopwords
    return text

def sentiment_analysis(reviews):
    classifier = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
    reviews['label'] = reviews['cleaned_text'].apply(lambda x: classifier(x)[0]['label'])
    return reviews

def create_feature_vectors(data):
    # Generate n-grams (bi-grams and tri-grams in this case)
    def generate_ngrams(text):
        words = text.split()
        return ['_'.join(gram) for gram in list(ngrams(words, 2)) + list(ngrams(words, 3))]
    
    tokenized_reviews = data['cleaned_text'].apply(generate_ngrams)
    model = Word2Vec(tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)
    review_features = np.array([np.mean([model.wv[word] for word in words if word in model.wv]
                           or [np.zeros(100)], axis=0)
                           for words in tokenized_reviews])
    return review_features

def expectation_labeling(reviews):
    def match(text):
        dic_clusters = {}
        dic_clusters["BASIC_NEED"] = ["satisfied with", "not working", "not even does basic", "won't use anymore", "pathetic support", "pathetic", "worth for money", "5 star deserving" , "value buy", "Good", "no problems", "easy to use", "easy", "worst", "cheated"]
        dic_clusters["DIFFERENTIATOR_NEED"] = ["could be better with", "good to have", "pricey", "compared to", "deserve to", "like", "switch brand", "buy other", "but not", "poor", "Gotta have", "great", "elegant"]
        for val in dic_clusters["BASIC_NEED"]:
            if val.lower() in text.lower():
                return "BASIC_NEED"
        for val in dic_clusters["DIFFERENTIATOR_NEED"]:
            if val.lower() in text.lower():
                return "DIFFERENTIATOR_NEED"
        return 'NO_MATCH'
    reviews['expectation_label'] = reviews['cleaned_text'].apply(lambda x: match(x))
    return reviews
    

def train_model(features, labels):
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
    classifier = RandomForestClassifier()
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    print(classification_report(y_test, y_pred))
    return y_pred, y_test

def make_recommendation(y_pred):
    positive_reviews = np.sum(y_pred == '5 stars') + np.sum(y_pred == '4 stars')
    negative_reviews = np.sum(y_pred == '1 star') + np.sum(y_pred == '2 stars')
    total_reviews = len(y_pred)
    
    print(f"Positive reviews: {positive_reviews}")
    print(f"Negative reviews: {negative_reviews}")
    
    if positive_reviews / total_reviews >= 0.6:
        return "Recommended to buy."
    elif negative_reviews / total_reviews >= 0.6:
        return "Not recommended to buy."
    else:
        return "Purchase decision is unclear."

# def k_means_clustering(data):
#     def generate_ngrams(text):
#         words = text.split()
#         return ['_'.join(gram) for gram in list(ngrams(words, 2)) + list(ngrams(words, 3))]
    
#     tokenized_reviews = data['cleaned_text'].apply(generate_ngrams)
#     print(tokenized_reviews)
#     kmeans = KMeans(n_clusters=2)
#     kmeans.fit(tokenized_reviews)
#     plt.scatter(x, y, c=kmeans.labels_)
#     plt.show()


def main():
    product_id = "B0C6F1GT12"#input("Enter the Amazon product ID: ")
    reviews = fetch_reviews(product_id)
    reviews['cleaned_text'] = reviews['review'].apply(clean_text)
    reviews = sentiment_analysis(reviews)
    features = create_feature_vectors(reviews)
    reviews = expectation_labeling(reviews)
    # print(features)
    print(reviews)
    y_pred, _ = train_model(features, reviews['label'])
    recommendation = make_recommendation(y_pred)
    print(recommendation)
    expectation_pred, _ = train_model(features, reviews['expectation_label'])
    print(expectation_pred)
    # k_means_clustering(reviews)


main()
